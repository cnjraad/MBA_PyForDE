[{"abstract":"As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.","ENTRYTYPE":"inproceedings","keywords":"Data Quality Dimensions, Big Data characteristics, Data Quality, Big Data","pages":null,"year":"2016","title":"Defining Big Data","ID":"10.1145\/3010089.3010090","author":"Emmanuel, Isitor and Stanier, Clare","doi":"10.1145\/3010089.3010090"},{"abstract":"Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company\u2019s core business data, using typically large datasets. However, data that doesn\u2019t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI\u2026). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to \u201csmartizing\u201d data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.","ENTRYTYPE":"inproceedings","keywords":"Big Data, Smart Data, Data Quality","pages":"19\u201324","year":"2018","title":"From Big Data to Smart Data: A Data Quality Perspective","ID":"10.1145\/3281022.3281026","author":"Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario","doi":"10.1145\/3281022.3281026"},{"abstract":"Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.","ENTRYTYPE":"inproceedings","keywords":"OLAP over big data, privacy of big data, big data posting, big data","pages":"198\u2013203","year":"2013","title":"Big Data: A Research Agenda","ID":"10.1145\/2513591.2527071","author":"Cuzzocrea, Alfredo and Sacc\\`{a}, Domenico and Ullman, Jeffrey D.","doi":"10.1145\/2513591.2527071"},{"abstract":"Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.","ENTRYTYPE":"inproceedings","keywords":"Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework","pages":"289-292","year":"2017","title":"A Big Data Framework for Electric Power Data Quality Assessment","ID":"8332632","author":"Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun","doi":"10.1109\/WISA.2017.29"},{"abstract":"In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.","ENTRYTYPE":"inproceedings","keywords":"Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing","pages":"498-501","year":"2017","title":"Big Data Pre-Processing: Closing the Data Quality Enforcement Loop","ID":"8029366","author":"Taleb, Ikbal and Serhani, Mohamed Adel","doi":"10.1109\/BigDataCongress.2017.73"},{"abstract":"Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called \u201cSakdas\u201d this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.","ENTRYTYPE":"inproceedings","keywords":"Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline","pages":"1-4","year":"2020","title":"Sakdas: A Python Package for Data Profiling and Data Quality Auditing","ID":"9245455","author":"Loetpipatwanich, Sakda and Vichitthamaros, Preecha","doi":"10.1109\/IBDAP50342.2020.9245455"},{"abstract":"In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.","ENTRYTYPE":"article","keywords":"Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data","pages":"132-147","year":"2021","title":"SparkDQ: Efficient generic big data quality management on distributed data-parallel computation","ID":"GU2021132","author":"Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang","doi":"https:\/\/doi.org\/10.1016\/j.jpdc.2021.05.012"},{"abstract":"Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.","ENTRYTYPE":"article","keywords":"Data quality-aware queries, Big data computing, Empirical evaluation","pages":"114858","year":"2021","title":"Experimenting with big data computing for scaling data quality-aware query processing","ID":"CISNEROSCABRERA2021114858","author":"Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris","doi":"https:\/\/doi.org\/10.1016\/j.eswa.2021.114858"},{"abstract":"The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.","ENTRYTYPE":"article","keywords":"Credit card, Validation, LUHN, Big data, Bank","pages":"532-537","year":"2022","title":"Perspective of anomaly detection in big data for data quality improvement","ID":"KESKAR2022532","author":"Vinaya Keskar and Jyoti Yadav and Ajay Kumar","doi":"https:\/\/doi.org\/10.1016\/j.matpr.2021.05.597"}]