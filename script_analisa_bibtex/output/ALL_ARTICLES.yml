- ENTRYTYPE: article
  ID: GU2021132
  abstract: In the big data era, large amounts of data are under generation and accumulation
    in various industries. However, users usually feel hindered by the data quality
    issues when extracting values from the big data. Thus, data quality issues are
    gaining more and more attention from data quality management analysts. Cutting-edge
    solutions like data ETL, data cleaning, and data quality monitoring systems have
    many deficiencies in capability and efficiency, making it difficult to cope with
    complicated situations on big data. These problems inspire us to build SparkDQ,
    a generic distributed data quality management model and framework that provides
    a series of data quality detection and repair interfaces. Users can quickly build
    custom tasks of data quality computing for various needs by utilizing these interfaces.
    In addition, SparkDQ implements a set of algorithms that in a parallel manner
    with optimizations. These algorithms aim at various data quality goals. We also
    propose several system-level optimizations, including the job-level optimization
    with multi-task execution scheduling and the data-level optimization with data
    state caching. The experimental evaluation shows that the proposed distributed
    algorithms in SparkDQ run up to 12 times faster compared to the corresponding
    stand-alone serial and multi-thread algorithms. Compared with the cutting-edge
    distributed data quality solution Apache Griffin, SparkDQ has more features, and
    its execution time is only around half of Apache Griffin on average. SparkDQ achieves
    near-linear data and node scalability.
  author: Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and
    Chunfeng Yuan and Yihua Huang
  doi: https://doi.org/10.1016/j.jpdc.2021.05.012
  issn: 0743-7315
  journal: Journal of Parallel and Distributed Computing
  keywords: Parallel data quality algorithms, Distributed system, Data quality management
    system, Multi-tasks scheduling, Big data
  pages: 132-147
  title: 'SparkDQ: Efficient generic big data quality management on distributed data-parallel
    computation'
  url: https://www.sciencedirect.com/science/article/pii/S0743731521001246
  volume: '156'
  year: '2021'
- ENTRYTYPE: article
  ID: CISNEROSCABRERA2021114858
  abstract: Combining query processing techniques with data quality management approaches
    enables enforcement of quality constraints, such as timeliness, accuracy and completeness,
    as part of ad-hoc query specification and execution, improving the quality of
    query results. Despite the emergence of novel data quality processing tools, there
    is a dearth of studies assessing performance and scalability in the execution
    of data quality assessment tasks during query processing. This paper reports on
    an empirical study aiming to investigate the extent to which a big data computing
    framework (Spark) can offer significant gains in performance and scalability when
    executing data quality querying tasks over a range of computational platforms
    including a single commodity multi-core machine and a cluster-based platform for
    a wide range of workloads. Our results show that substantial performance and scalability
    gains can be obtained by using optimized data science libraries combined with
    the parallel and distributed capabilities of big data computing. We also provide
    guidelines on choosing the appropriate computational infrastructure for executing
    DQ-aware queries.
  author: Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio
    and Pedro Sampaio and Anastasios Gounaris
  doi: https://doi.org/10.1016/j.eswa.2021.114858
  issn: 0957-4174
  journal: Expert Systems with Applications
  keywords: Data quality-aware queries, Big data computing, Empirical evaluation
  pages: '114858'
  title: Experimenting with big data computing for scaling data quality-aware query
    processing
  url: https://www.sciencedirect.com/science/article/pii/S0957417421002992
  volume: '178'
  year: '2021'
- ENTRYTYPE: article
  ID: KESKAR2022532
  abstract: The period of Big Data examination has started in many businesses inside
    created nations. With expanding headway of Internet technology, expanding measures
    of data are spilling into contemporary associations. Data are getting bigger and
    more muddled because of the nonstop age of data from numerous gadgets and sources.
    In this investigation, we have examined the banking area's inconsistencies because
    of big data technology, inconsistencies in credit card and afterwards the path
    how to remove these inconsistencies.
  author: Vinaya Keskar and Jyoti Yadav and Ajay Kumar
  doi: https://doi.org/10.1016/j.matpr.2021.05.597
  issn: 2214-7853
  journal: 'Materials Today: Proceedings'
  keywords: Credit card, Validation, LUHN, Big data, Bank
  note: CMAE'21
  pages: 532-537
  title: Perspective of anomaly detection in big data for data quality improvement
  url: https://www.sciencedirect.com/science/article/pii/S2214785321042243
  volume: '51'
  year: '2022'
- ENTRYTYPE: inproceedings
  ID: '8332632'
  abstract: Since a low-quality data may influence the effectiveness and reliability
    of applications, data quality is required to be guaranteed. Data quality assessment
    is considered as the foundation of the promotion of data quality, so it is essential
    to access the data quality before any other data related activities. In the electric
    power industry, more and more electric power data is continuously accumulated,
    and many electric power applications have been developed based on these data.
    In China, the power grid has many special characteristic, traditional big data
    assessment frameworks cannot be directly applied. Therefore, a big data framework
    for electric power data quality assessment is proposed. Based on big data techniques,
    the framework can accumulate both the real-time data and the history data, provide
    an integrated computation environment for electric power big data assessment,
    and support the storage of different types of data.
  author: Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun
  booktitle: 2017 14th Web Information Systems and Applications Conference (WISA)
  doi: 10.1109/WISA.2017.29
  issn: ''
  keywords: Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data
    quality;electric power data;data quality assessment;big data;framework
  month: Nov
  number: ''
  pages: 289-292
  title: A Big Data Framework for Electric Power Data Quality Assessment
  volume: ''
  year: '2017'
- ENTRYTYPE: inproceedings
  ID: '8029366'
  abstract: In the Big Data Era, data is the core for any governmental, institutional,
    and private organization. Efforts were geared towards extracting highly valuable
    insights that cannot happen if data is of poor quality. Therefore, data quality
    (DQ) is considered as a key element in Big data processing phase. In this stage,
    low quality data is not penetrated to the Big Data value chain. This paper, addresses
    the data quality rules discovery (DQR) after the evaluation of quality and prior
    to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately
    target the pre-processing activities based on quality requirements. We defined,
    a set of pre-processing activities associated with data quality dimensions (DQD's)
    to automatize the DQR generation process. Rules optimization are applied on validated
    rules to avoid multi-passes pre-processing activities and eliminates duplicate
    rules. Conducted experiments showed an increased quality scores after applying
    the discovered and optimized DQR's on data.
  author: Taleb, Ikbal and Serhani, Mohamed Adel
  booktitle: 2017 IEEE International Congress on Big Data (BigData Congress)
  doi: 10.1109/BigDataCongress.2017.73
  issn: ''
  keywords: Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality
    Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing
  month: June
  number: ''
  pages: 498-501
  title: 'Big Data Pre-Processing: Closing the Data Quality Enforcement Loop'
  volume: ''
  year: '2017'
- ENTRYTYPE: inproceedings
  ID: '9245455'
  abstract: "Data Profiling and data quality management become a more significant\
    \ part of data engineering, which an essential part of ensuring that the system\
    \ delivers quality information to users. In the last decade, data quality was\
    \ considered to need more managing. Especially in the big data era that the data\
    \ comes from many sources, many data types, and an enormous amount. Thus it makes\
    \ the managing of data quality is more difficult and complicated. The traditional\
    \ system was unable to respond as needed. The data quality managing software for\
    \ big data was developed but often found in a high-priced, difficult to customize\
    \ as needed, and mostly provide as GUI, which is challenging to integrate with\
    \ other systems. From this problem, we have developed an opensource package for\
    \ data quality managing. By using Python programming language, Which is a programming\
    \ language that is widely used in the scientific and engineering field today.\
    \ Because it is a programming language that is easy to read syntax, small, and\
    \ has many additional packages to integrate. The software developed here is called\
    \ \u201CSakdas\u201D this package has been divided into three parts. The first\
    \ part deals with data profiling provide a set of data analyses to generate a\
    \ data profile, and this profile will help to define the data quality rules. The\
    \ second part deals with data quality auditing that users can set their own data\
    \ quality rules for data quality measurement. The final part deals with data visualizing\
    \ that provides data profiling and data auditing report to improve the data quality.\
    \ The results of the profiling and auditing services, the user can specify both\
    \ the form of a report for self-review. Or in the form of JSON for use in post-process\
    \ automation."
  author: Loetpipatwanich, Sakda and Vichitthamaros, Preecha
  booktitle: 2020 1st International Conference on Big Data Analytics and Practices
    (IBDAP)
  doi: 10.1109/IBDAP50342.2020.9245455
  issn: ''
  keywords: Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data
    Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline
  month: Sep.
  number: ''
  pages: 1-4
  title: 'Sakdas: A Python Package for Data Profiling and Data Quality Auditing'
  volume: ''
  year: '2020'
- ENTRYTYPE: article
  ID: GU2021132
  abstract: In the big data era, large amounts of data are under generation and accumulation
    in various industries. However, users usually feel hindered by the data quality
    issues when extracting values from the big data. Thus, data quality issues are
    gaining more and more attention from data quality management analysts. Cutting-edge
    solutions like data ETL, data cleaning, and data quality monitoring systems have
    many deficiencies in capability and efficiency, making it difficult to cope with
    complicated situations on big data. These problems inspire us to build SparkDQ,
    a generic distributed data quality management model and framework that provides
    a series of data quality detection and repair interfaces. Users can quickly build
    custom tasks of data quality computing for various needs by utilizing these interfaces.
    In addition, SparkDQ implements a set of algorithms that in a parallel manner
    with optimizations. These algorithms aim at various data quality goals. We also
    propose several system-level optimizations, including the job-level optimization
    with multi-task execution scheduling and the data-level optimization with data
    state caching. The experimental evaluation shows that the proposed distributed
    algorithms in SparkDQ run up to 12 times faster compared to the corresponding
    stand-alone serial and multi-thread algorithms. Compared with the cutting-edge
    distributed data quality solution Apache Griffin, SparkDQ has more features, and
    its execution time is only around half of Apache Griffin on average. SparkDQ achieves
    near-linear data and node scalability.
  author: Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and
    Chunfeng Yuan and Yihua Huang
  doi: https://doi.org/10.1016/j.jpdc.2021.05.012
  issn: 0743-7315
  journal: Journal of Parallel and Distributed Computing
  keywords: Parallel data quality algorithms, Distributed system, Data quality management
    system, Multi-tasks scheduling, Big data
  pages: 132-147
  title: 'SparkDQ: Efficient generic big data quality management on distributed data-parallel
    computation'
  url: https://www.sciencedirect.com/science/article/pii/S0743731521001246
  volume: '156'
  year: '2021'
- ENTRYTYPE: article
  ID: CISNEROSCABRERA2021114858
  abstract: Combining query processing techniques with data quality management approaches
    enables enforcement of quality constraints, such as timeliness, accuracy and completeness,
    as part of ad-hoc query specification and execution, improving the quality of
    query results. Despite the emergence of novel data quality processing tools, there
    is a dearth of studies assessing performance and scalability in the execution
    of data quality assessment tasks during query processing. This paper reports on
    an empirical study aiming to investigate the extent to which a big data computing
    framework (Spark) can offer significant gains in performance and scalability when
    executing data quality querying tasks over a range of computational platforms
    including a single commodity multi-core machine and a cluster-based platform for
    a wide range of workloads. Our results show that substantial performance and scalability
    gains can be obtained by using optimized data science libraries combined with
    the parallel and distributed capabilities of big data computing. We also provide
    guidelines on choosing the appropriate computational infrastructure for executing
    DQ-aware queries.
  author: Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio
    and Pedro Sampaio and Anastasios Gounaris
  doi: https://doi.org/10.1016/j.eswa.2021.114858
  issn: 0957-4174
  journal: Expert Systems with Applications
  keywords: Data quality-aware queries, Big data computing, Empirical evaluation
  pages: '114858'
  title: Experimenting with big data computing for scaling data quality-aware query
    processing
  url: https://www.sciencedirect.com/science/article/pii/S0957417421002992
  volume: '178'
  year: '2021'
- ENTRYTYPE: article
  ID: KESKAR2022532
  abstract: The period of Big Data examination has started in many businesses inside
    created nations. With expanding headway of Internet technology, expanding measures
    of data are spilling into contemporary associations. Data are getting bigger and
    more muddled because of the nonstop age of data from numerous gadgets and sources.
    In this investigation, we have examined the banking area's inconsistencies because
    of big data technology, inconsistencies in credit card and afterwards the path
    how to remove these inconsistencies.
  author: Vinaya Keskar and Jyoti Yadav and Ajay Kumar
  doi: https://doi.org/10.1016/j.matpr.2021.05.597
  issn: 2214-7853
  journal: 'Materials Today: Proceedings'
  keywords: Credit card, Validation, LUHN, Big data, Bank
  note: CMAE'21
  pages: 532-537
  title: Perspective of anomaly detection in big data for data quality improvement
  url: https://www.sciencedirect.com/science/article/pii/S2214785321042243
  volume: '51'
  year: '2022'
