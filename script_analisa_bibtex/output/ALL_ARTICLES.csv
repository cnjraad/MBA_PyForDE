series;location;keywords;numpages;pages;booktitle;abstract;doi;url;address;publisher;isbn;year;title;author;ENTRYTYPE;ID;articleno;month;journal;issn;number;volume;issue_date;note
BDE 2019;Hong Kong, Hong Kong;Big data, Big data quality metrics, Sentiment analysis;8;36–43;Proceedings of the 2019 International Conference on Big Data Engineering;In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.;10.1145/3341620.3341629;https://doi.org/10.1145/3341620.3341629;New York, NY, USA;Association for Computing Machinery;9781450360913;2019;Big Data Quality Metrics for Sentiment Analysis Approaches;El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi;inproceedings;10.1145/3341620.3341629;;;;;;;;
SITA'20;Rabat, Morocco;Big Data, Data Quality, Quality Models, Data Quality evaluation;6;;Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications;In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.;10.1145/3419604.3419803;https://doi.org/10.1145/3419604.3419803;New York, NY, USA;Association for Computing Machinery;9781450377331;2020;Towards a Data Quality Assessment in Big Data;Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir;inproceedings;10.1145/3419604.3419803;16;;;;;;;
BDAW '16;Blagoevgrad, Bulgaria;Big Data characteristics, Big Data, Data Quality Dimensions, Data Quality;6;;Proceedings of the International Conference on Big Data and Advanced Wireless Technologies;As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.;10.1145/3010089.3010090;https://doi.org/10.1145/3010089.3010090;New York, NY, USA;Association for Computing Machinery;9781450347792;2016;Defining Big Data;Emmanuel, Isitor and Stanier, Clare;inproceedings;10.1145/3010089.3010090;5;;;;;;;
EnSEmble 2018;Lake Buena Vista, FL, USA;Data Quality, Smart Data, Big Data;6;19–24;Proceedings of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software Engineering;Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.;10.1145/3281022.3281026;https://doi.org/10.1145/3281022.3281026;New York, NY, USA;Association for Computing Machinery;9781450360548;2018;From Big Data to Smart Data: A Data Quality Perspective;Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario;inproceedings;10.1145/3281022.3281026;;;;;;;;
IDEAS '13;Barcelona, Spain;big data posting, big data, OLAP over big data, privacy of big data;6;198–203;"Proceedings of the 17th International Database Engineering &amp; Applications Symposium";Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.;10.1145/2513591.2527071;https://doi.org/10.1145/2513591.2527071;New York, NY, USA;Association for Computing Machinery;9781450320252;2013;Big Data: A Research Agenda;Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.;inproceedings;10.1145/2513591.2527071;;;;;;;;
ICCBDC 2017;London, United Kingdom;big data, NoSQL, databases, data warehouse, NewSQL, MapReduce;5;6–10;Proceedings of the 2017 International Conference on Cloud and Big Data Computing;Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.;10.1145/3141128.3141139;https://doi.org/10.1145/3141128.3141139;New York, NY, USA;Association for Computing Machinery;9781450353434;2017;Big Data and New Data Warehousing Approaches;Pti\v{c}ek, Marina and Vrdoljak, Boris;inproceedings;10.1145/3141128.3141139;;;;;;;;
Data4U '14;Hangzhou, China;Big data, data integration, diverse data sources;4;25–28;"Proceedings of the First International Workshop on Bringing the Value of ""Big Data"" to Users (Data4U 2014)";"In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and ""wrangle"" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.";10.1145/2658840.2658845;https://doi.org/10.1145/2658840.2658845;New York, NY, USA;Association for Computing Machinery;9781450331869;2014;Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics;Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.;inproceedings;10.1145/2658840.2658845;;;;;;;;
ICBDE '18;Honolulu, HI, USA;Data Management, Big Data;5;52–56;Proceedings of the 2018 International Conference on Big Data and Education;In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.;10.1145/3206157.3206166;https://doi.org/10.1145/3206157.3206166;New York, NY, USA;Association for Computing Machinery;9781450363587;2018;The 10 Vs, Issues and Challenges of Big Data;Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz;inproceedings;10.1145/3206157.3206166;;;;;;;;
BigDataScience '14;Beijing, China;Visualization, Mining, R, Hadoop, Big data;6;;Proceedings of the 2014 International Conference on Big Data Science and Computing;Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.;10.1145/2640087.2644168;https://doi.org/10.1145/2640087.2644168;New York, NY, USA;Association for Computing Machinery;9781450328913;2014;Big Data Analysis with Interactive Visualization Using R Packages;Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi;inproceedings;10.1145/2640087.2644168;18;;;;;;;
ICBDC 2020;Chengdu, China;HDFS, Apache Hadoop, Big Data Technology, MapReduce;9;23–31;Proceedings of the 2020 5th International Conference on Big Data and Computing;In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.;10.1145/3404687.3404694;https://doi.org/10.1145/3404687.3404694;New York, NY, USA;Association for Computing Machinery;9781450375474;2020;A Comprehensive Overview of BIG DATA Technologies: A Survey;Raza, Muhammad Umair and XuJian, Zhao;inproceedings;10.1145/3404687.3404694;;;;;;;;
WABD 2021;Fuzhou, China;Personalized service, Big data, Marketing;4;79–82;2021 Workshop on Algorithm and Big Data;In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.;10.1145/3456389.3456390;https://doi.org/10.1145/3456389.3456390;New York, NY, USA;Association for Computing Machinery;9781450389945;2021;Opportunities and Challenges of Marketing in the Context of Big Data;Cao, Shuangshuang;inproceedings;10.1145/3456389.3456390;;;;;;;;
BIGDSE '16;Austin, Texas;innovation, architecture landscape, ecosystem, value discovery, big data, value engineering, energy industry;7;44–50;Proceedings of the 2nd International Workshop on BIG Data Software Engineering;"This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from ""bounded rationality"" for problem solving to ""expandable rationality"" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call ""eBay in the Grid"".";10.1145/2896825.2896837;https://doi.org/10.1145/2896825.2896837;New York, NY, USA;Association for Computing Machinery;9781450341523;2016;Toward Big Data Value Engineering for Innovation;Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy;inproceedings;10.1145/2896825.2896837;;;;;;;;
BIGDSE '15;Florence, Italy;big data, embedded case study methodology, collaborative practice research, software architecture, data system design methods, system engineering;7;44–50;Proceedings of the First International Workshop on BIG Data Software Engineering;Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.;;;;IEEE Press;;2015;Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm;Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha;inproceedings;10.5555/2819289.2819302;;;;;;;;
ICBDC 2019;Guangzhou, China;large-scale complex systems, effectiveness, evaluation, big data;5;72–76;Proceedings of the 2019 4th International Conference on Big Data and Computing;With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.;10.1145/3335484.3335545;https://doi.org/10.1145/3335484.3335545;New York, NY, USA;Association for Computing Machinery;9781450362788;2019;Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data;Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang;inproceedings;10.1145/3335484.3335545;;;;;;;;
;;Big Data, requirements engineering, software reference architecture, quality assurance, Big Data systems, software engineering;39;;;Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.;10.1145/3408314;https://doi.org/10.1145/3408314;New York, NY, USA;Association for Computing Machinery;;2020;Big Data Systems: A Software Engineering Perspective;Davoudian, Ali and Liu, Mengchi;article;10.1145/3408314;110;sep;ACM Comput. Surv.;0360-0300;5;53;September 2021;
DOLAP '15;Melbourne, Australia;big data, nosql, database design;4;35–38;Proceedings of the ACM Eighteenth International Workshop on Data Warehousing and OLAP;It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.;10.1145/2811222.2811235;https://doi.org/10.1145/2811222.2811235;New York, NY, USA;Association for Computing Machinery;9781450337854;2015;Big Data Design;Abell\'{o}, Alberto;inproceedings;10.1145/2811222.2811235;;;;;;;;
iiWAS2019;Munich, Germany;Big Data integration, Source reliability, Big Data Source Selection, Data quality;6;611–616;"Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services";Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.;10.1145/3366030.3366121;https://doi.org/10.1145/3366030.3366121;New York, NY, USA;Association for Computing Machinery;9781450371797;2019;Data Source Selection in Big Data Context;Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim;inproceedings;10.1145/3366030.3366121;;;;;;;;
DIVANet '14;Montreal, QC, Canada;big data, cloud computing;6;139–144;Proceedings of the Fourth ACM International Symposium on Development and Analysis of Intelligent Vehicular Networks and Applications;This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.;10.1145/2656346.2656358;https://doi.org/10.1145/2656346.2656358;New York, NY, USA;Association for Computing Machinery;9781450330282;2014;Big Data Architecture Evolution: 2014 and Beyond;Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel;inproceedings;10.1145/2656346.2656358;;;;;;;;
;;;5;50–54;;New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.;10.1145/2331042.2331058;https://doi.org/10.1145/2331042.2331058;New York, NY, USA;Association for Computing Machinery;;2012;Interactive Analysis of Big Data;Heer, Jeffrey and Kandel, Sean;article;10.1145/2331042.2331058;;sep;XRDS;1528-4972;1;19;Fall 2012;
BigMine '12;Beijing, China;;5;7–11;Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications;"Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of ""Big Data"" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of ""Big Data"" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving ""Big Data"", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.";10.1145/2351316.2351318;https://doi.org/10.1145/2351316.2351318;New York, NY, USA;Association for Computing Machinery;9781450315470;2012;Big Data, Big Business: Bridging the Gap;Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James;inproceedings;10.1145/2351316.2351318;;;;;;;;
CODASPY '15;San Antonio, Texas, USA;privacy, security, big data;2;279–280;Proceedings of the 5th ACM Conference on Data and Application Security and Privacy;This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.;10.1145/2699026.2699136;https://doi.org/10.1145/2699026.2699136;New York, NY, USA;Association for Computing Machinery;9781450331913;2015;Big Data Security and Privacy;Thuraisingham, Bhavani;inproceedings;10.1145/2699026.2699136;;;;;;;;
BDIOT2017;London, United Kingdom;Manufacturing, Project Prioritization, Framework, Project Selection, Industrial Big Data;5;6–10;Proceedings of the International Conference on Big Data and Internet of Thing;The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.;10.1145/3175684.3175687;https://doi.org/10.1145/3175684.3175687;New York, NY, USA;Association for Computing Machinery;9781450354301;2017;A Data-Based Method for Industrial Big Data Project Prioritization;Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina;inproceedings;10.1145/3175684.3175687;;;;;;;;
BDC '14;;Big Data, Ensemble learning, Bayesian network, Scientific workflow, Kepler, Distributed computing, Hadoop;10;16–25;Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing;In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.;10.1109/BDC.2014.10;https://doi.org/10.1109/BDC.2014.10;USA;IEEE Computer Society;9781479918973;2014;A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning;Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay;inproceedings;10.1109/BDC.2014.10;;;;;;;;
;;;8;37–44;"Proceedings of the 22nd International Database Engineering &amp; Applications Symposium";The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.;;https://doi.org/10.1145/3216122.3216124;New York, NY, USA;Association for Computing Machinery;9781450365277;2018;Quality Awareness for a Successful Big Data Exploitation;Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica;inbook;10.1145/3216122.3216124;;;;;;;;
BDCAT '16;Shanghai, China;data management, vs challenges, big data, data lifecycle, data complexity, data organization;7;100–106;Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies;"A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.";10.1145/3006299.3006311;https://doi.org/10.1145/3006299.3006311;New York, NY, USA;Association for Computing Machinery;9781450346177;2016;Towards a Comprehensive Data Lifecycle Model for Big Data Environments;Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva;inproceedings;10.1145/3006299.3006311;;;;;;;;
FOSE 2014;Hyderabad, India;Operational Data, Data Quality, Statistics, Data Science, Game Theory, Analytics, Data Engineering;15;85–99;Future of Software Engineering Proceedings;" Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles. ";10.1145/2593882.2593889;https://doi.org/10.1145/2593882.2593889;New York, NY, USA;Association for Computing Machinery;9781450328654;2014;Engineering Big Data Solutions;Mockus, Audris;inproceedings;10.1145/2593882.2593889;;;;;;;;
;;big data, analytics, tactical environment, algorithms, cloud computing;4;86–89;;We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.;10.1145/2627534.2627561;https://doi.org/10.1145/2627534.2627561;New York, NY, USA;Association for Computing Machinery;;2014;Tactical Big Data Analytics: Challenges, Use Cases, and Solutions;Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason;article;10.1145/2627534.2627561;;apr;SIGMETRICS Perform. Eval. Rev.;0163-5999;4;41;March 2014;
CCGrid '18;Washington, District of Columbia;;7;675–681;Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing;"This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the ""pedigree"" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.";10.1109/CCGRID.2018.00100;https://doi.org/10.1109/CCGRID.2018.00100;;IEEE Press;9781538658154;2018;Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments;Cuzzocrea, Alfredo and Damiani, Ernesto;inproceedings;10.1109/CCGRID.2018.00100;;;;;;;;
dg.o '13;Quebec, Canada;big data, open government;10;1–10;Proceedings of the 14th Annual International Conference on Digital Government Research;"The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From ""smart"" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.";10.1145/2479724.2479730;https://doi.org/10.1145/2479724.2479730;New York, NY, USA;Association for Computing Machinery;9781450320573;2013;Big Data and E-Government: Issues, Policies, and Recommendations;Bertot, John Carlo and Choi, Heeyoon;inproceedings;10.1145/2479724.2479730;;;;;;;;
ICCBD '20;Taichung, Taiwan;Industrie 4.0, Big data, Multi-stage assembly, Digital transformation;7;48–54;2020 the 3rd International Conference on Computing and Big Data;The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.;10.1145/3418688.3418697;https://doi.org/10.1145/3418688.3418697;New York, NY, USA;Association for Computing Machinery;9781450387866;2020;The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0;Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza;inproceedings;10.1145/3418688.3418697;;;;;;;;
;;5V challenges, multimedia analysis, mobile multimedia, data mining, machine learning, Big data analytics, survey, indexing, retrieval, multimedia databases;34;;;With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.;10.1145/3150226;https://doi.org/10.1145/3150226;New York, NY, USA;Association for Computing Machinery;;2018;Multimedia Big Data Analytics: A Survey;Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.;article;10.1145/3150226;10;jan;ACM Comput. Surv.;0360-0300;1;51;January 2019;
ICC '16;Cambridge, United Kingdom;Protecting Big Data, Big Data Analytics, Big Data, Warehousing Big Data;7;;Proceedings of the International Conference on Internet of Things and Cloud Computing;This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.;10.1145/2896387.2900335;https://doi.org/10.1145/2896387.2900335;New York, NY, USA;Association for Computing Machinery;9781450340632;2016;Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges;Cuzzocrea, Alfredo;inproceedings;10.1145/2896387.2900335;14;;;;;;;
dg.o '18;Delft, The Netherlands;use, electronic census, big data challenges, census big data, public value creation, big data analytics, cross case analysis;10;;Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age;"Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.";10.1145/3209281.3209372;https://doi.org/10.1145/3209281.3209372;New York, NY, USA;Association for Computing Machinery;9781450365260;2018;Census Big Data Analytics Use: International Cross Case Analysis;Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.;inproceedings;10.1145/3209281.3209372;10;;;;;;;
ICSDE'18;Rabat, Morocco;Multidimensional approach, Big Data, Hadoop, MapReduce jobs, Data placing, Intelligent processing;6;42–47;Proceedings of the 2nd International Conference on Smart Digital Environment;Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.;10.1145/3289100.3289108;https://doi.org/10.1145/3289100.3289108;New York, NY, USA;Association for Computing Machinery;9781450365079;2018;Towards Efficient Big Data: Hadoop Data Placing and Processing;Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick;inproceedings;10.1145/3289100.3289108;;;;;;;;
;;classification, data quality, machine learning, data streams, deep learning, label noise, big data;;;;Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.;10.1145/3492546;https://doi.org/10.1145/3492546;New York, NY, USA;Association for Computing Machinery;;2021;A Survey on Classifying Big Data with Label Noise;Johnson, Justin M and Khoshgoftaar, Taghi M;article;10.1145/3492546;;oct;J. Data and Information Quality;1936-1955;;;;Just Accepted
ICBDC 2020;Chengdu, China;Locomotive system, Key technology, Railway, Big data, Application platform;7;6–12;Proceedings of the 2020 5th International Conference on Big Data and Computing;In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.;10.1145/3404687.3404693;https://doi.org/10.1145/3404687.3404693;New York, NY, USA;Association for Computing Machinery;9781450375474;2020;Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System;Xin, Li and Tianyun, Shi and Xiaoning, Ma;inproceedings;10.1145/3404687.3404693;;;;;;;;
;;co-design, education, community engagement, big data;4;41–44;;University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.;10.1145/3331651.3331659;https://doi.org/10.1145/3331651.3331659;New York, NY, USA;Association for Computing Machinery;;2019;Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods;Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie;article;10.1145/3331651.3331659;;may;SIGKDD Explor. Newsl.;1931-0145;1;21;June 2019;
BDET 2018;Chengdu, China;insurance industry, Financial technology, platform architecture, time and space data, big data platform;5;31–35;Proceedings of the 2018 International Conference on Big Data Engineering and Technology;With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.;10.1145/3297730.3297743;https://doi.org/10.1145/3297730.3297743;New York, NY, USA;Association for Computing Machinery;9781450365826;2018;Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example;Liu, Yi and Peng, Jiawen and Yu, Zhihao;inproceedings;10.1145/3297730.3297743;;;;;;;;
ICBDR 2019;Cergy-Pontoise, France;Theory of evidence, Data Reliability, IoT, Smart City;6;18–23;Proceedings of the 2019 3rd International Conference on Big Data Research;Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.;10.1145/3372454.3372478;https://doi.org/10.1145/3372454.3372478;New York, NY, USA;Association for Computing Machinery;9781450372015;2019;Assessing Reliability of Big Data Stream for Smart City;Puangpontip, Supadchaya and Hewett, Rattikorn;inproceedings;10.1145/3372454.3372478;;;;;;;;
BDET 2018;Chengdu, China;fuzzy decision tree, Target data optimization, fuzzy case-based reasoning, big data-streaming;5;26–30;Proceedings of the 2018 International Conference on Big Data Engineering and Technology;How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.;10.1145/3297730.3297731;https://doi.org/10.1145/3297730.3297731;New York, NY, USA;Association for Computing Machinery;9781450365826;2018;Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System;Chen, Rui-Yang;inproceedings;10.1145/3297730.3297731;;;;;;;;
MET '19;Montreal, Quebec, Canada;data quality, metamorphic data relations, big data, metamorphic testing, quality assessment;8;76–83;Proceedings of the 4th International Workshop on Metamorphic Testing;In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.;10.1109/MET.2019.00019;https://doi.org/10.1109/MET.2019.00019;;IEEE Press;;2019;Addressing Data Quality Problems with Metamorphic Data Relations;Auer, Florian and Felderer, Michael;inproceedings;10.1109/MET.2019.00019;;;;;;;;
SIGMOD '15;Melbourne, Victoria, Australia;customer retention, big data, telco churn prediction;12;607–618;Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data;We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.;10.1145/2723372.2742794;https://doi.org/10.1145/2723372.2742794;New York, NY, USA;Association for Computing Machinery;9781450327589;2015;Telco Churn Prediction with Big Data;Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia;inproceedings;10.1145/2723372.2742794;;;;;;;;
ICEGOV '17;New Delhi AA, India;data linkage, ex-post policy evaluation, Big data, counterfactuals;4;228–231;Proceedings of the 10th International Conference on Theory and Practice of Electronic Governance;"The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the ""ceteris paribus"" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.";10.1145/3047273.3047377;https://doi.org/10.1145/3047273.3047377;New York, NY, USA;Association for Computing Machinery;9781450348256;2017;Exploiting Big Data for Evaluation Studies;Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald;inproceedings;10.1145/3047273.3047377;;;;;;;;
ICSIE '19;Cairo, Egypt;Benefits, Challenges, Analytics, Big Data;4;196–199;Proceedings of the 2019 8th International Conference on Software and Information Engineering;"The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered";10.1145/3328833.3328841;https://doi.org/10.1145/3328833.3328841;New York, NY, USA;Association for Computing Machinery;9781450361057;2019;Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?;Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.;inproceedings;10.1145/3328833.3328841;;;;;;;;
CompSysTech '19;Ruse, Bulgaria;Emerging Architectures, Big Data Value Chain, Smart City, GATE Platform, Big Data;8;261–268;Proceedings of the 20th International Conference on Computer Systems and Technologies;Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.;10.1145/3345252.3345282;https://doi.org/10.1145/3345252.3345282;New York, NY, USA;Association for Computing Machinery;9781450371490;2019;Conceptual Architecture of GATE Big Data Platform;Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena;inproceedings;10.1145/3345252.3345282;;;;;;;;
;;Data quality, data quality assessment, requirements for metrics, data quality metrics;32;;;Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.;10.1145/3148238;https://doi.org/10.1145/3148238;New York, NY, USA;Association for Computing Machinery;;2018;Requirements for Data Quality Metrics;Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael;article;10.1145/3148238;12;jan;J. Data and Information Quality;1936-1955;2;9;June 2017;
;;;12;7–18;;Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.;10.1145/2854006.2854008;https://doi.org/10.1145/2854006.2854008;New York, NY, USA;Association for Computing Machinery;;2015;Data Quality: From Theory to Practice;Fan, Wenfei;article;10.1145/2854006.2854008;;dec;SIGMOD Rec.;0163-5808;3;44;September 2015;
ACM TUR-C '17;Shanghai, China;data science, course architecture, big data;6;;Proceedings of the ACM Turing 50th Celebration Conference - China;Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.;10.1145/3063955.3063968;https://doi.org/10.1145/3063955.3063968;New York, NY, USA;Association for Computing Machinery;9781450348737;2017;The Design of Course Architecture for Big Data;Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie;inproceedings;10.1145/3063955.3063968;13;;;;;;;
ICARCSET '15;Unnao, India;exploration, Big data, unstructured data, Data analytics, infringement, application, monitor;5;;"Proceedings of the 2015 International Conference on Advanced Research in Computer Science Engineering &amp; Technology (ICARCSET 2015)";"As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.";10.1145/2743065.2743099;https://doi.org/10.1145/2743065.2743099;New York, NY, USA;Association for Computing Machinery;9781450334419;2015;Perspectives, Motivations and Implications Of Big Data Analytics;Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.;inproceedings;10.1145/2743065.2743099;34;;;;;;;
ICACS '17;Jeju Island, Republic of Korea;big data strategy, Big data analytics, big data challenges, big data success factors;5;88–92;Proceedings of the International Conference on Algorithms, Computing and Systems;This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.;10.1145/3127942.3127961;https://doi.org/10.1145/3127942.3127961;New York, NY, USA;Association for Computing Machinery;9781450352840;2017;Determinants of Big Data Adoption and Success;Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel;inproceedings;10.1145/3127942.3127961;;;;;;;;
;;unstructured Big Data, content management, Big Data analytics, data quality, machine learning, text analytics;19;;;The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.;10.1145/3461015;https://doi.org/10.1145/3461015;New York, NY, USA;Association for Computing Machinery;;2022;Data and Process Quality Evaluation in a Textual Big Data Archiving System;Fugini, Mariagrazia and Finocchi, Jacopo;article;10.1145/3461015;2;mar;J. Comput. Cult. Herit.;1556-4673;1;15;February 2022;
CSAE '18;Hohhot, China;data quality, Data governance, governance framework;5;;Proceedings of the 2nd International Conference on Computer Science and Application Engineering;With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.;10.1145/3207677.3278000;https://doi.org/10.1145/3207677.3278000;New York, NY, USA;Association for Computing Machinery;9781450365123;2018;Research and Application of Enterprise Big Data Governance;Ke, Changwen and Wang, Kuisheng;inproceedings;10.1145/3207677.3278000;29;;;;;;;
ICEGOV '14;Guimaraes, Portugal;business intelligence, ICT salary profile, big data analytics;2;450–451;Proceedings of the 8th International Conference on Theory and Practice of Electronic Governance;This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.;10.1145/2691195.2691196;https://doi.org/10.1145/2691195.2691196;New York, NY, USA;Association for Computing Machinery;9781605586113;2014;Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective;Ramasamy, Ramachandran;inproceedings;10.1145/2691195.2691196;;;;;;;;
DATA '19;Dubai, United Arab Emirates;big data, data processing, OLAP, data mining, machin learning;5;;Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems;In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.;10.1145/3368691.3368717;https://doi.org/10.1145/3368691.3368717;New York, NY, USA;Association for Computing Machinery;9781450372848;2019;Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector;Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi;inproceedings;10.1145/3368691.3368717;26;;;;;;;
DOLAP '13;San Francisco, California, USA;sql, mapreduce, parallel algorithms, dbms, big data;8;85–92;Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP;"Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the ""big data analytics"" trend.";10.1145/2513190.2513198;https://doi.org/10.1145/2513190.2513198;New York, NY, USA;Association for Computing Machinery;9781450324120;2013;Can We Analyze Big Data inside a DBMS?;Ordonez, Carlos;inproceedings;10.1145/2513190.2513198;;;;;;;;
COINS '19;Crete, Greece;data storage, data characteristics, data generation, Big Data;6;19–24;Proceedings of the International Conference on Omni-Layer Intelligent Systems;Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.;10.1145/3312614.3312623;https://doi.org/10.1145/3312614.3312623;New York, NY, USA;Association for Computing Machinery;9781450366403;2019;The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges;Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul;inproceedings;10.1145/3312614.3312623;;;;;;;;
;;quality assessment, linked data, Data quality;32;;;"The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.";10.1145/2992786;https://doi.org/10.1145/2992786;New York, NY, USA;Association for Computing Machinery;;2016;Luzzu—A Methodology and Framework for Linked Data Quality Assessment;"Debattista, Jeremy and Auer, S\""{O}ren and Lange, Christoph";article;10.1145/2992786;4;oct;J. Data and Information Quality;1936-1955;1;8;November 2016;
IDEAS 2021;Montreal, QC, Canada;Big data, Veracity;9;157–165;"25th International Database Engineering &amp; Applications Symposium";Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.;10.1145/3472163.3472195;https://doi.org/10.1145/3472163.3472195;New York, NY, USA;Association for Computing Machinery;9781450389914;2021;Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity;Sahri, Soror and Moussa, Rim;inproceedings;10.1145/3472163.3472195;;;;;;;;
CIKM '16;Indianapolis, Indiana, USA;telco big data, regression models, localization;10;439–448;Proceedings of the 25th ACM International on Conference on Information and Knowledge Management;It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.;10.1145/2983323.2983345;https://doi.org/10.1145/2983323.2983345;New York, NY, USA;Association for Computing Machinery;9781450340731;2016;City-Scale Localization with Telco Big Data;Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia;inproceedings;10.1145/2983323.2983345;;;;;;;;
PCI '16;Patras, Greece;signal processing techniques, stochastic approximation, convex optimization, statistical learning tools, big data;6;;Proceedings of the 20th Pan-Hellenic Conference on Informatics;Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.;10.1145/3003733.3003767;https://doi.org/10.1145/3003733.3003767;New York, NY, USA;Association for Computing Machinery;9781450347891;2016;Signal Processing Techniques Restructure The Big Data Era;Petrou, Charilaos and Paraskevas, Michael;inproceedings;10.1145/3003733.3003767;52;;;;;;;
;;fact checking, data fusion, information extraction, data quality, Truth discovery;3;;;;10.1145/2935753;https://doi.org/10.1145/2935753;New York, NY, USA;Association for Computing Machinery;;2016;Veracity of Big Data: Challenges of Cross-Modal Truth Discovery;Berti-Equille, Laure and Ba, Mouhamadou Lamine;article;10.1145/2935753;12;aug;J. Data and Information Quality;1936-1955;3;7;September 2016;
CSAI 2017;Jakarta, Indonesia;Measurement, Metric, Big Data Analytics, Software;5;265–269;Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence;Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.;10.1145/3168390.3168425;https://doi.org/10.1145/3168390.3168425;New York, NY, USA;Association for Computing Machinery;9781450353922;2017;Measurement Metric Proposed For Big Data Analytics System;Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano;inproceedings;10.1145/3168390.3168425;;;;;;;;
ICISE 2021;Shanghai, China;Pre-Processing, Classification using SVM, Map-reduce, Big Data, Fault data detection;6;1–6;2021 the 6th International Conference on Information Systems Engineering;In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.;10.1145/3503928.3503929;https://doi.org/10.1145/3503928.3503929;New York, NY, USA;Association for Computing Machinery;9781450385220;2021;Big Data: Finding Frequencies of Faulty Multimedia Data;Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha;inproceedings;10.1145/3503928.3503929;;;;;;;;
;;cloud IoT services, cloud computing in IoT, big data 2.0, IoT big data survey, V’s challenges for IoT big data, IoT big data;59;;;Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.;10.1145/3419634;https://doi.org/10.1145/3419634;New York, NY, USA;Association for Computing Machinery;;2020;A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions;Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n;article;10.1145/3419634;131;dec;ACM Comput. Surv.;0360-0300;6;53;November 2021;
SIGMOD '13;New York, New York, USA;machine learning, hadoop, data mining, offline processing, big data, data pipeline;10;1125–1134;Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data;The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.;10.1145/2463676.2463707;https://doi.org/10.1145/2463676.2463707;New York, NY, USA;Association for Computing Machinery;9781450320375;2013;The Big Data Ecosystem at LinkedIn;Sumbaly, Roshan and Kreps, Jay and Shah, Sam;inproceedings;10.1145/2463676.2463707;;;;;;;;
IDEAS 2021;Montreal, QC, Canada;Representational Theory of Measurement,, Validity, Measurement Hierarchical Model, Quality Characteristics (V's), Big Data;7;285–291;"25th International Database Engineering &amp; Applications Symposium";Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.;10.1145/3472163.3472171;https://doi.org/10.1145/3472163.3472171;New York, NY, USA;Association for Computing Machinery;9781450389914;2021;Rigorous Measurement Model for Validity of Big Data: MEGA Approach;Bhardwaj, Dave and Ormandjieva, Olga;inproceedings;10.1145/3472163.3472171;;;;;;;;
;;;9;35–43;;We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.;10.1145/3186549.3186559;https://doi.org/10.1145/3186549.3186559;New York, NY, USA;Association for Computing Machinery;;2018;Data Quality: The Role of Empiricism;Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh;article;10.1145/3186549.3186559;;feb;SIGMOD Rec.;0163-5808;4;46;December 2017;
KDD '14;New York, New York, USA;big data, missing value, clustering;10;651–660;Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining;Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.;10.1145/2623330.2623615;https://doi.org/10.1145/2623330.2623615;New York, NY, USA;Association for Computing Machinery;9781450329569;2014;Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla;Anagnostopoulos, Christos and Triantafillou, Peter;inproceedings;10.1145/2623330.2623615;;;;;;;;
;;empirical software engineering., business goals, big data applications, big data requirements engineering, empirical studies;6;1–6;;Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.;10.1145/3178315.3178323;https://doi.org/10.1145/3178315.3178323;New York, NY, USA;Association for Computing Machinery;;2018;Requirements Engineering in the Context of Big Data Applications;Arruda, Darlan;article;10.1145/3178315.3178323;;mar;SIGSOFT Softw. Eng. Notes;0163-5948;1;43;January 2018;
dg.o '20;Seoul, Republic of Korea;Privacy, Big Data, Personal Data Protection, Governance, Anonymisation;11;185–195;The 21st Annual International Conference on Digital Government Research;In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy. ;10.1145/3396956.3398253;https://doi.org/10.1145/3396956.3398253;New York, NY, USA;Association for Computing Machinery;9781450387910;2020;Big Data, Anonymisation and Governance to Personal Data Protection;Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique;inproceedings;10.1145/3396956.3398253;;;;;;;;
DOLAP '13;San Francisco, California, USA;olap, big data, big multidimensional data, data warehousing;4;67–70;Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP;In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.;10.1145/2513190.2517828;https://doi.org/10.1145/2513190.2517828;New York, NY, USA;Association for Computing Machinery;9781450324120;2013;Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions;Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol;inproceedings;10.1145/2513190.2517828;;;;;;;;
MEDES '20;Virtual Event, United Arab Emirates;Software containers, Big Data workflows, Domain-specific languages;8;76–83;Proceedings of the 12th International Conference on Management of Digital EcoSystems;Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.;10.1145/3415958.3433082;https://doi.org/10.1145/3415958.3433082;New York, NY, USA;Association for Computing Machinery;9781450381154;2020;Scalable Execution of Big Data Workflows Using Software Containers;Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru;inproceedings;10.1145/3415958.3433082;;;;;;;;
dg.o '17;Staten Island, NY, USA;enterprise architecture, BOLD, infrastructure, big data, ICT-architecture, open data, e-government;6;505–510;Proceedings of the 18th Annual International Conference on Digital Government Research;Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.;10.1145/3085228.3085275;https://doi.org/10.1145/3085228.3085275;New York, NY, USA;Association for Computing Machinery;9781450353175;2017;Enterprise Architectures for Supporting the Adoption of Big Data;Gong, Yiwei and Janssen, Marijn;inproceedings;10.1145/3085228.3085275;;;;;;;;
EBIMCS 2020;Wuhan, China;Information-oriented education, Smart education, Big data;6;228–233;Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science;The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.;10.1145/3453187.3453340;https://doi.org/10.1145/3453187.3453340;New York, NY, USA;Association for Computing Machinery;9781450389099;2020;Research on Smart Education Service Platform Based on Big Data;Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona;inproceedings;10.1145/3453187.3453340;;;;;;;;
ICCAD '15;Austin, TX, USA;analytics, manufacturing, Big data, semiconductor;5;776–780;Proceedings of the IEEE/ACM International Conference on Computer-Aided Design;"Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an ""old-fashioned"" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.";;;;IEEE Press;9781467383899;2015;"Modern Big Data Analytics for ""Old-Fashioned"" Semiconductor Industry Applications";Zhu, Yada and Xiong, Jinjun;inproceedings;10.5555/2840819.2840927;;;;;;;;
CAIH2020;Taiyuan, China;Electronic Health Record, SOA, EMR, Big Data, Healthcare;7;170–176;Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare;"Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates ""difficulty and expensive"" problem effectively.";10.1145/3433996.3434027;https://doi.org/10.1145/3433996.3434027;New York, NY, USA;Association for Computing Machinery;9781450388641;2020;The Planning and Construction of Healthcare Big Data Platform;Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun;inproceedings;10.1145/3433996.3434027;;;;;;;;
BigSpatial '14;Dallas, Texas;data warehouse, MapReduce, spatial analytics, GIS, database;4;11–14;Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data;The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.;10.1145/2676536.2676538;https://doi.org/10.1145/2676536.2676538;New York, NY, USA;Association for Computing Machinery;9781450331326;2014;High Performance Integrated Spatial Big Data Analytics;Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng;inproceedings;10.1145/2676536.2676538;;;;;;;;
CSAE 2020;Sanya, China;Opening and sharing of data resource, Scientific data management, Big data, Scientific data;6;;Proceedings of the 4th International Conference on Computer Science and Application Engineering;"Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.";10.1145/3424978.3425010;https://doi.org/10.1145/3424978.3425010;New York, NY, USA;Association for Computing Machinery;9781450377720;2020;Research on Scientific Data Management in Big Data Era;Man, Rui and Zhou, Guomin and Fan, Jingchao;inproceedings;10.1145/3424978.3425010;32;;;;;;;
;;internet of things, graph systems, big data, Dynamic data quality, relational systems;3;;;;10.1145/2998575;https://doi.org/10.1145/2998575;New York, NY, USA;Association for Computing Machinery;;2017;An Introduction to Dynamic Data Quality Challenges;Labouseur, Alan G. and Matheus, Carolyn C.;article;10.1145/2998575;6;jan;J. Data and Information Quality;1936-1955;2;8;February 2017;
ICBDR 2020;Tokyo, Japan;AIS data, Ship encounter, Visualization, Fuzzy theory;7;94–100;2020 the 4th International Conference on Big Data Research (ICBDR'20);As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.;10.1145/3445945.3445962;https://doi.org/10.1145/3445945.3445962;New York, NY, USA;Association for Computing Machinery;9781450387750;2020;A Visual Method for Ship Close Encounter Pattern Recognition Based on Fuzzy Theory and Big Data Intelligence;Zhao, Liangbin and Fu, Xiuju;inproceedings;10.1145/3445945.3445962;;;;;;;;
AICCC '21;Kyoto, Japan;Government audit, Influencing factors, Big data analysis capability;7;172–178;2021 4th Artificial Intelligence and Cloud Computing Conference;The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.;10.1145/3508259.3508284;https://doi.org/10.1145/3508259.3508284;New York, NY, USA;Association for Computing Machinery;9781450384162;2021;Research on Influencing Factors of Government Audit Big Data Capability;Sun, Yu and Niu, Yanfang and Lu, Le;inproceedings;10.1145/3508259.3508284;;;;;;;;
;;big data analytics application, SLA metrics, service level agreement, SLA, service layer, Big data;40;;;Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.;10.1145/3383464;https://doi.org/10.1145/3383464;New York, NY, USA;Association for Computing Machinery;;2020;SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study;Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv;article;10.1145/3383464;46;jun;ACM Comput. Surv.;0360-0300;3;53;May 2021;
;;4V challenges, Big data analytics, survey, clinical decision support, computational health informatics, data mining, machine learning;36;;;The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.;10.1145/2932707;https://doi.org/10.1145/2932707;New York, NY, USA;Association for Computing Machinery;;2016;Computational Health Informatics in the Big Data Age: A Survey;Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.;article;10.1145/2932707;12;jun;ACM Comput. Surv.;0360-0300;1;49;March 2017;
;;;14;6–19;;"The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on ""big data"". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life ""in the trenches"" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall ""big picture"" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as ""plumbing"". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.";10.1145/2481244.2481247;https://doi.org/10.1145/2481244.2481247;New York, NY, USA;Association for Computing Machinery;;2013;Scaling Big Data Mining Infrastructure: The Twitter Experience;Lin, Jimmy and Ryaboy, Dmitriy;article;10.1145/2481244.2481247;;apr;SIGKDD Explor. Newsl.;1931-0145;2;14;December 2012;
ICDEL 2020;Beijing, China;Big data background, mode, foreign exchange management;4;162–165;Proceedings of the 5th International Conference on Distance Education and Learning;The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.;10.1145/3402569.3409041;https://doi.org/10.1145/3402569.3409041;New York, NY, USA;Association for Computing Machinery;9781450377546;2020;Research on Foreign Exchange Management Model Based on Big Data;Han, Ping;inproceedings;10.1145/3402569.3409041;;;;;;;;
SSDBM '15;La Jolla, California;;11;;Proceedings of the 27th International Conference on Scientific and Statistical Database Management;The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.;10.1145/2791347.2791380;https://doi.org/10.1145/2791347.2791380;New York, NY, USA;Association for Computing Machinery;9781450337090;2015;Privacy-Preserving Big Data Publishing;Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken;inproceedings;10.1145/2791347.2791380;26;;;;;;;
EITCE 2021;Xiamen, China;Tourism prediction, Multi-source big data, Data fusion technology;7;1030–1036;Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering;In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.;10.1145/3501409.3501593;https://doi.org/10.1145/3501409.3501593;New York, NY, USA;Association for Computing Machinery;9781450384322;2021;Tourism Prediction Based on Multi-Source Big Data Fusion Technology;Diao, Yanhua;inproceedings;10.1145/3501409.3501593;;;;;;;;
CSAI2019;Normal, IL, USA;Construction, Analysis of big data, Data asset, Shared, Distributed;5;54–58;Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence;A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.;10.1145/3374587.3374650;https://doi.org/10.1145/3374587.3374650;New York, NY, USA;Association for Computing Machinery;9781450376273;2019;Construction and Application of Big Data Analysis Platform for Enterprise;Shen, Shaoyi and Li, Bin and Li, Situo;inproceedings;10.1145/3374587.3374650;;;;;;;;
SAICSIT '15;Stellenbosch, South Africa;Big Data Analytics, South Africa, Big Data, Technology Adoption;9;;Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists;"The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA";10.1145/2815782.2815793;https://doi.org/10.1145/2815782.2815793;New York, NY, USA;Association for Computing Machinery;9781450336833;2015;Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry;Malaka, Iman and Brown, Irwin;inproceedings;10.1145/2815782.2815793;27;;;;;;;
IMMS 2019;Chengdu, China;performance management, Big data, human resources;6;12–17;Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences;With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.;10.1145/3357292.3357302;https://doi.org/10.1145/3357292.3357302;New York, NY, USA;Association for Computing Machinery;9781450371445;2019;Big Data Informatization Applied to Optimization of Human Resource Performance Management;Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang;inproceedings;10.1145/3357292.3357302;;;;;;;;
ICIIT 2018;Ha Noi, Viet Nam;Big Data, Method for confirming information property rights, Information property index, Confirmation of Information Property;6;59–64;Proceedings of the 2018 International Conference on Intelligent Information Technology;The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.;10.1145/3193063.3193069;https://doi.org/10.1145/3193063.3193069;New York, NY, USA;Association for Computing Machinery;9781450363785;2018;An Overview of Techniques for Confirming Big Data Property Rights;Cheng, Susu and Zhao, Haijun;inproceedings;10.1145/3193063.3193069;;;;;;;;
ACSW '18;Brisband, Queensland, Australia;big data stream processing, resource optimization, big data, cloud computing, cost optimization, SDN;11;;Proceedings of the Australasian Computer Science Week Multiconference;Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.;10.1145/3167918.3167924;https://doi.org/10.1145/3167918.3167924;New York, NY, USA;Association for Computing Machinery;9781450354363;2018;A Survey on Big Data Stream Processing in SDN Supported Cloud Environment;Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav;inproceedings;10.1145/3167918.3167924;12;;;;;;;
ICGDA '18;Prague, Czech Republic;data science, geospatial big data, big data, data mining, machine learning;5;98–102;Proceedings of the International Conference on Geoinformatics and Data Analysis;Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.;10.1145/3220228.3220236;https://doi.org/10.1145/3220228.3220236;New York, NY, USA;Association for Computing Machinery;9781450364454;2018;A New Data Science Framework for Analysing and Mining Geospatial Big Data;Saraee, Mo and Silva, Charith;inproceedings;10.1145/3220228.3220236;;;;;;;;
MEDES '19;Limassol, Cyprus;Data quality, Data Quality Characteristics, Data Quality requirements, Conceptual model, Citizen science;8;166–173;Proceedings of the 11th International Conference on Management of Digital EcoSystems;Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.;10.1145/3297662.3365797;https://doi.org/10.1145/3297662.3365797;New York, NY, USA;Association for Computing Machinery;9781450362382;2019;Integrating Data Quality Requirements to Citizen Science Application Design;Musto, Jiri and Dahanayake, Ajantha;inproceedings;10.1145/3297662.3365797;;;;;;;;
;;;5;56–60;;Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.;10.1145/2094114.2094129;https://doi.org/10.1145/2094114.2094129;New York, NY, USA;Association for Computing Machinery;;2012;The Meaningful Use of Big Data: Four Perspectives -- Four Challenges;Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri;article;10.1145/2094114.2094129;;jan;SIGMOD Rec.;0163-5808;4;40;December 2011;
ICIAI '18;Shanghai, China;tech big data, combination, mining, Hadoop;5;59–63;Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence;With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.;10.1145/3194206.3194229;https://doi.org/10.1145/3194206.3194229;New York, NY, USA;Association for Computing Machinery;9781450363457;2018;Based on Hadoop's Tech Big Data Combination and Mining Technology Framework;Zhichao, Xu and Jiandong, Zhao and Huan, Huang;inproceedings;10.1145/3194206.3194229;;;;;;;;
;;;;;;"The report from the workshop, ""Big Data, Big Decisions for Government, Business and Society,"" makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.";;;USA;National Science Foundation;;2015;Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop;Markus, M. Lynne and Topi, Heikki;techreport;10.5555/2849516;;;;;;;;
ICIIT '19;Da, Nang, Viet Nam;regulatory construction, Data assets, big data trading platform;6;107–112;Proceedings of the 2019 4th International Conference on Intelligent Information Technology;As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.;10.1145/3321454.3321474;https://doi.org/10.1145/3321454.3321474;New York, NY, USA;Association for Computing Machinery;9781450366335;2019;Research on the Construction of Big Data Trading Platform in China;Yu, Bangbo and Zhao, Haijun;inproceedings;10.1145/3321454.3321474;;;;;;;;
ICEMIS '18;Istanbul, Turkey;Data Warehouse, Business Intelligence, Cloud Computing, Big Data;9;;"Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018";Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.;10.1145/3234698.3234723;https://doi.org/10.1145/3234698.3234723;New York, NY, USA;Association for Computing Machinery;9781450363921;2018;Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges;El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha;inproceedings;10.1145/3234698.3234723;25;;;;;;;
CSAE 2019;Sanya, China;Big data architecture, Data analysis, Crop germplasm resources, Data management;7;;Proceedings of the 3rd International Conference on Computer Science and Application Engineering;Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.;10.1145/3331453.3361308;https://doi.org/10.1145/3331453.3361308;New York, NY, USA;Association for Computing Machinery;9781450362948;2019;Construction and Implementation of Big Data Framework for Crop Germplasm Resources;Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing;inproceedings;10.1145/3331453.3361308;27;;;;;;;
;;"Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework";;289-292;2017 14th Web Information Systems and Applications Conference (WISA);Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.;10.1109/WISA.2017.29;;;;;2017;A Big Data Framework for Electric Power Data Quality Assessment;Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun;inproceedings;8332632;;Nov;;;;;;
;;"Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing";;498-501;2017 IEEE International Congress on Big Data (BigData Congress);In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.;10.1109/BigDataCongress.2017.73;;;;;2017;Big Data Pre-Processing: Closing the Data Quality Enforcement Loop;Taleb, Ikbal and Serhani, Mohamed Adel;inproceedings;8029366;;June;;;;;;
;;"Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline";;1-4;2020 1st International Conference on Big Data Analytics and Practices (IBDAP);Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.;10.1109/IBDAP50342.2020.9245455;;;;;2020;Sakdas: A Python Package for Data Profiling and Data Quality Auditing;Loetpipatwanich, Sakda and Vichitthamaros, Preecha;inproceedings;9245455;;Sep.;;;;;;
;;"Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment";;248-252;2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA);Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.;10.1109/ICCCBDA.2018.8386521;;;;;2018;Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory;Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai;inproceedings;8386521;;April;;;;;;
;;"Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data";;69-74;2018 International Conference on Innovations in Information Technology (IIT);Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.;10.1109/INNOVATIONS.2018.8605945;;;;;2018;Big Data Quality Assessment Model for Unstructured Data;Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida;inproceedings;8605945;;Nov;;2325-5498;;;;
;;"Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality";;149-152;2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA);This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.;10.1109/ICBDA.2017.8078796;;;;;2017;Some key problems of data management in army data engineering based on big data;HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang;inproceedings;8078796;;March;;;;;;
;;"Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale";;2644-2653;2015 IEEE International Conference on Big Data (Big Data);"A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the ""truth about Big Data"" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.";10.1109/BigData.2015.7364064;;;;;2015;Big data, big data quality problem;Becker, David and King, Trish Dunn and McMullen, Bill;inproceedings;7364064;;Oct;;;;;;
;;"Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning";;791-792;2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA);Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.;10.1109/DSAA49011.2020.00119;;;;;2020;Big Data Quality Prediction on Banking Applications: Extended Abstract;Wong, Ka Yee. and Wong, Raymond K.;inproceedings;9260067;;Oct;;;;;;
;;"Big Data;Quality Measurement;Quality Model;Quality Assurance";;1-3;2019 International Conference on Big Data and Computational Intelligence (ICBDCI);"Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.";10.1109/ICBDCI.2019.8686099;;;;;2019;Big Data Quality Challenges;Abdallah, Mohammad;inproceedings;8686099;;Feb;;;;;;
;;"Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling";;759-765;2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld);Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.;10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122;;;;;2016;Big Data Quality: A Quality Dimensions Evaluation;Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik;inproceedings;7816918;;July;;;;;;
;;"Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality";;2654-2660;2015 IEEE International Conference on Big Data (Big Data);Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.;10.1109/BigData.2015.7364065;;;;;2015;Data quality issues in big data;Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.;inproceedings;7364065;;Oct;;;;;;
;;"Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing";;559-563;2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon);Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.;10.1109/COMITCon.2019.8862267;;;;;2019;Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application;Juneja, Ashish and Das, Nripendra Narayan;inproceedings;8862267;;Feb;;;;;;
;;"Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance";;116-121;2016 IEEE International Conference on Big Data (Big Data);Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.;10.1109/BigData.2016.7840595;;;;;2016;Antecedents of big data quality: An empirical examination in financial service organizations;Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn;inproceedings;7840595;;Dec;;;;;;
;;"Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality";;68-84;;Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.;10.26599/BDMA.2019.9020019;;;;;2020;Mining conditional functional dependency rules on big data;Li, Mingda and Wang, Hongzhi and Li, Jianzhong;article;8935096;;March;Big Data Mining and Analytics;2096-0654;1;3;;
;;"Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system";;1-7;2017 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computed, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI);With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.;10.1109/UIC-ATC.2017.8397554;;;;;2017;Data quality in big data processing: Issues, solutions and open problems;Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin;inproceedings;8397554;;Aug;;;;;;
;;"Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool";;5977-5979;2019 IEEE International Conference on Big Data (Big Data);The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.;10.1109/BigData47090.2019.9006294;;;;;2019;QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications;Arruda, Darlan and Madhavji, Nazim H.;inproceedings;9006294;;Dec;;;;;;
;;"Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis";;1-6;2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD);Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.;10.1109/ICABCD.2018.8465129;;;;;2018;Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis;Juddoo, Suraj and George, Carlisle;inproceedings;8465129;;Aug;;;;;;
;;"Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics";;2612-2620;2015 IEEE International Conference on Big Data (Big Data);Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.;10.1109/BigData.2015.7364060;;;;;2015;Evaluation of data quality of multisite electronic health record data for secondary analysis;Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.;inproceedings;7364060;;Oct;;;;;;
;;"Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science";;3260-3266;2019 IEEE International Conference on Big Data (Big Data);"Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.";10.1109/BigData47090.2019.9006358;;;;;2019;Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams;Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka;inproceedings;9006358;;Dec;;;;;;
;;"Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest";;3101-3103;IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium;In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.;10.1109/IGARSS39084.2020.9323615;;;;;2020;A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System;Han, Weiguo and Jochum, Matthew;inproceedings;9323615;;Sep.;;2153-7003;;;;
;;"Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science";;2913-2922;2019 IEEE International Conference on Big Data (Big Data);Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.;10.1109/BigData47090.2019.9006187;;;;;2019;DQA: Scalable, Automated and Interactive Data Quality Advisor;Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.;inproceedings;9006187;;Dec;;;;;;
;;"Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.";;357-362;2019 12th International Conference on Developments in eSystems Engineering (DeSE);"Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.";10.1109/DeSE.2019.00072;;;;;2019;Data Quality Management for Big Data Applications;Khaleel, Majida Yaseen and Hamad, Murtadha M.;inproceedings;9073586;;Oct;;2161-1351;;;;
;;"Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics";;1-9;2015 International Conference on Computing, Communication and Security (ICCCS);"Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.";10.1109/CCCS.2015.7374131;;;;;2015;Overview of data quality challenges in the context of Big Data;Juddoo, Suraj;inproceedings;7374131;;Dec;;;;;;
;;"Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids";;1-5;2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM);"Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.";10.1109/IWCIM.2015.7347061;;;;;2015;Computing data quality indicators on Big Data streams using a CEP;Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce;inproceedings;7347061;;Oct;;;;;;
;;"Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm";;395-398;2021 International Conference on Wireless Communications and Smart Grid (ICWCSG);As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.;10.1109/ICWCSG53609.2021.00085;;;;;2021;"Research on smart grid big data amp;#x2019;s curve mean clustering algorithm for edge-cloud collaborative application";Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da;inproceedings;9616552;;Aug;;;;;;
;;"Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors";;1-5;2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS);The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.;10.1109/ICDS47004.2019.8942297;;;;;2019;Towards a multi-agents model for errors detection and correction in big data flows;Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz;inproceedings;8942297;;Oct;;;;;;
;;"Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality";;2974-2979;2017 IEEE International Conference on Big Data (Big Data);Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.;10.1109/BigData.2017.8258267;;;;;2017;My (fair) big data;Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil;inproceedings;8258267;;Dec;;;;;;
;;"Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning";;498-501;2018 IEEE 5th International Congress on Information Science and Technology (CiSt);Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.;10.1109/CIST.2018.8596389;;;;;2018;A Study of Handling Missing Data Methods for Big Data;Ezzine, Imane and Benhlima, Laila;inproceedings;8596389;;Oct;;2327-1884;;;;
;;"Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing";;191-198;2015 IEEE International Congress on Big Data;With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.;10.1109/BigDataCongress.2015.35;;;;;2015;Big Data Pre-processing: A Quality Framework;Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel;inproceedings;7207219;;June;;2379-7703;;;;
;;"Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment";;869-872;2015 International Conference on Intelligent Transportation, Big Data and Smart City;This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.;10.1109/ICITBS.2015.220;;;;;2015;The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology;Chenran, Xiong and Youde, Wu;inproceedings;7384166;;Dec;;;;;;
;;"Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration";;1-8;2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS);Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.;10.1109/ICECOCS50124.2020.9314391;;;;;2020;Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security;Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif;inproceedings;9314391;;Dec;;;;;;
;;"Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata";;2863-2871;2016 IEEE International Conference on Big Data (Big Data);As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.;10.1109/BigData.2016.7840935;;;;;2016;Bad big data science;Haug, Frank S.;inproceedings;7840935;;Dec;;;;;;
;;"Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data";;166-173;2018 IEEE International Congress on Big Data (BigData Congress);With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.;10.1109/BigDataCongress.2018.00029;;;;;2018;Big Data Quality: A Survey;Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida;inproceedings;8457745;;July;;;;;;
;;"Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality";;1142-1143;2013 IEEE 16th International Conference on Computational Science and Engineering;The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.;10.1109/CSE.2013.168;;;;;2013;Information Governance, Big Data and Data Quality;Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo;inproceedings;6755349;;Dec;;;;;;
;;"Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty";;36294-36299;;In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.;10.1109/ACCESS.2019.2904286;;;;;2019;Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea;Lee, Doyoung;article;8667300;;;IEEE Access;2169-3536;;7;;
;;"Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment";;155-162;2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData);Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.;10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28;;;;;2017;Towards a Data Quality Framework for Heterogeneous Data;Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib;inproceedings;8276745;;June;;;;;;
;;"Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics";;47-48;2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W);This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.;10.1109/DSN-W.2018.00023;;;;;2018;EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform;Blanquer, Ignacio and Meira, Wagner;inproceedings;8416208;;June;;2325-6664;;;;
;;"Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning";;58-66;2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM);Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.;10.1109/ELECOM49001.2020.9297009;;;;;2020;A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry;Juddoo, Suraj and George, Carlisle;inproceedings;9297009;;Nov;;;;;;
;;"Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software";;529-532;2017 IEEE International Congress on Big Data (BigData Congress);Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.;10.1109/BigDataCongress.2017.91;;;;;2017;Challenges of Software Testing for Astronomical Big Data;Zhou, Lixiao and Huang, Maohai;inproceedings;8029373;;June;;;;;;
;;"Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines";;1579-1586;2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC);"Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.";10.1109/COMPSAC51774.2021.00235;;;;;2021;Toward a Novel Measurement Framework for Big Data (MEGA);Bhardwaj, Dave and Ormandjieva, Olga;inproceedings;9529590;;July;;0730-3157;;;;
;;"Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias";;2612-2619;2017 IEEE International Conference on Big Data (Big Data);This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.;10.1109/BigData.2017.8258221;;;;;2017;Is data quality enough for a clinical decision?: Apply machine learning and avoid bias;Hee, Kim;inproceedings;8258221;;Dec;;;;;;
;;"Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design";;77-81;2020 International Conference on E-Commerce and Internet Technology (ECIT);In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.;10.1109/ECIT50008.2020.00026;;;;;2020;Design of Network Precision Marketing Based on Big Data Analysis Technology;Gan, Wenting;inproceedings;9134140;;April;;;;;;
;;"Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy";;1446-1450;2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC);In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.;10.1109/IAEAC50856.2021.9391025;;;;;2021;Research on Data Security in Big Data Cloud Computing Environment;Wang, Fengling and Wang, Han and Xue, Liang;inproceedings;9391025;;March;;2689-6621;;5;;
;;"Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality";;241-247;2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD);In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.;10.1109/ACIT-CSII-BCD.2017.49;;;;;2017;A Survey on Big Data Pre-processing;Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai;inproceedings;8787092;;July;;;;;;
;;"Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot";;5328-5329;2018 IEEE International Conference on Big Data (Big Data);Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.;10.1109/BigData.2018.8621924;;;;;2018;Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example;Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth;inproceedings;8621924;;Dec;;;;;;
;;"Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks";;1113-1122;2015 IEEE International Conference on Big Data (Big Data);Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.;10.1109/BigData.2015.7363865;;;;;2015;Online anomaly detection over Big Data streams;Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal;inproceedings;7363865;;Oct;;;;;;
;;"Big data;Cleaning;Computer science;Databases;Investment";;1446-1447;2016 IEEE 32nd International Conference on Data Engineering (ICDE);The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.;10.1109/ICDE.2016.7498367;;;;;2016;Big data quality - whose problem is it?;Sadiq, Shazia and Papotti, Paolo;inproceedings;7498367;;May;;;;;;
;;"Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement";;1-9;2021 International Conference on Information Systems and Advanced Technologies (ICISAT);Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.;10.1109/ICISAT54145.2021.9678209;;;;;2021;A survey on data quality: principles, taxonomies and comparison of approaches;Yalaoui, Mehdi and Boukhedouma, Saida;inproceedings;9678209;;Dec;;;;;;
;;"Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study";;281-286;2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService);With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.;10.1109/BigDataService.2017.44;;;;;2017;Big Data Validation Case Study;Xie, Chunli and Gao, Jerry and Tao, Chuanqi;inproceedings;7944952;;April;;;;;;
;;"Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control";;2327-2336;2021 IEEE International Conference on Big Data (Big Data);Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.;10.1109/BigData52589.2021.9671672;;;;;2021;Unsupervised Anomaly Detection in Data Quality Control;Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming;inproceedings;9671672;;Dec;;;;;;
;;"Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog";;2056-2065;2015 IEEE International Conference on Big Data (Big Data);This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.;10.1109/BigData.2015.7363987;;;;;2015;Business information modeling: A methodology for data-intensive projects, data science and big data governance;Priebe, Torsten and Markus, Stefan;inproceedings;7363987;;Oct;;;;;;
;;"Data Governance;Data Credibility;Data Traceability";;290-294;2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC);In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.;10.1109/DSC50466.2020.00051;;;;;2020;A data traceability method to improve data quality in a big data environment;Zhang, Guobao;inproceedings;9172875;;July;;;;;;
;;"Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment";;418-425;2016 IEEE International Congress on Big Data (BigData Congress);While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.;10.1109/BigDataCongress.2016.65;;;;;2016;An Hybrid Approach to Quality Evaluation across Big Data Value Chain;Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana;inproceedings;7584971;;June;;;;;;
;;"Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema";;3792-3799;2017 IEEE International Conference on Big Data (Big Data);"The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.";10.1109/BigData.2017.8258380;;;;;2017;Understanding data quality: Ensuring data quality by design in the rail industry;Fu, Qian and Easton, John M.;inproceedings;8258380;;Dec;;;;;;
;;"Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems";;74-79;2017 IEEE International Conference on Big Data (Big Data);We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.;10.1109/BigData.2017.8257913;;;;;2017;Enhancing data quality by cleaning inconsistent big RDF data;Benbernou, Salima and Ouziri, Mourad;inproceedings;8257913;;Dec;;;;;;
;;"Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service";;134-139;2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS);As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.;10.1109/BigDataSecurity-HPSC-IDS.2016.52;;;;;2016;Aspects of Data Cataloguing for Enterprise Data Platforms;Shanmugam, Srinivasan and Seshadri, Gokul;inproceedings;7502278;;April;;;;;;
;;"Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning";;200-205;2019 IEEE International Conference on Big Data (Big Data);Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.;10.1109/BigData47090.2019.9006446;;;;;2019;An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection;Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G;inproceedings;9006446;;Dec;;;;;;
;;"Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction";;210-215;2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService);Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.;10.1109/BigDataService.2016.37;;;;;2016;Big Data Quality: A Roadmap for Open Data;Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel;inproceedings;7474375;;March;;;;;;
;;"Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON";;468-482;;In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.;10.1109/TBDATA.2017.2725904;;;;;2021;A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment;Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman;article;7974765;;June;IEEE Transactions on Big Data;2332-7790;2;7;;
;;"Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation";;433-441;2016 IEEE Symposium on Service-Oriented System Engineering (SOSE);With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.;10.1109/SOSE.2016.63;;;;;2016;Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs;Gao, Jerry and Xie, Chunli and Tao, Chuanqi;inproceedings;7473058;;March;;;;;;
;;"Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science";;4844-4853;2018 IEEE International Conference on Big Data (Big Data);"""Big Data readiness"" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.";10.1109/BigData.2018.8622229;;;;;2018;A Path to Big Data Readiness;Austin, Claire C.;inproceedings;8622229;;Dec;;;;;;
;;"Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality";;1914-1923;2020 IEEE International Conference on Big Data (Big Data);The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).;10.1109/BigData50022.2020.9378148;;;;;2020;Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD;O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana;inproceedings;9378148;;Dec;;;;;;
;;"Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges";;100-106;2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT);A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.;;;;;;2016;Towards a Comprehensive Data LifeCycle Model for Big Data Environments;Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva;inproceedings;7877056;;Dec;;;;;;
;;"1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics";;43-48;2017 International Workshop on Big Data and Information Security (IWBIS);Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.;10.1109/IWBIS.2017.8275101;;;;;2017;Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics;Setiadi, Yazid and Uluwiyah, Ana;inproceedings;8275101;;Sep.;;;;;;
;;"Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality";;5900-5902;2021 IEEE International Conference on Big Data (Big Data);This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.;10.1109/BigData52589.2021.9671890;;;;;2021;QMLEx: Data Driven Digital Transformation in Marketing Analytics;Geronazzo, Angela and Ziegler, Markus;inproceedings;9671890;;Dec;;;;;;
;;"Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection";;2443-2454;;"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.";10.1109/JIOT.2021.3096637;;;;;2022;An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques;Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed;article;9481251;;Feb;IEEE Internet of Things Journal;2327-4662;3;9;;
;;"Cleaning;Big data;Measurement;Business;Software;Instruments;Industries";;1595-1602;2016 IEEE International Conference on Big Data (Big Data);Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.;10.1109/BigData.2016.7840769;;;;;2016;Data quality: Experiences and lessons from operationalizing big data;Ganapathi, Archana and Chen, Yanpei;inproceedings;7840769;;Dec;;;;;;
;;"Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis";;73-79;2015 Third International Conference on Advanced Cloud and Big Data;The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.;10.1109/CBD.2015.22;;;;;2015;An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data;Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun;inproceedings;7435456;;Oct;;;;;;
;;"Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control";;463-466;2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR);Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.;10.1109/MIPR.2019.00093;;;;;2019;A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control;Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita;inproceedings;8695373;;March;;;;;;
;;"Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics";;363-367;2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE);With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.;10.1109/ICISCAE52414.2021.9590700;;;;;2021;Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model;Du, Jinming;inproceedings;9590700;;Sep.;;;;;;
;;"Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning";;1-9;2020 29th International Conference on Computer Communications and Networks (ICCCN);Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.;10.1109/ICCCN49398.2020.9209633;;;;;2020;Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments;Byabazaire, John and O’Hare, Gregory and Delaney, Declan;inproceedings;9209633;;Aug;;2637-9430;;;;
;;"Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis";;384-391;2017 IEEE International Congress on Big Data (BigData Congress);Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.;10.1109/BigDataCongress.2017.56;;;;;2017;Data Governance Framework for Big Data Implementation with a Case of Korea;Kim, Hee Young and Cho, June-Suh;inproceedings;8029349;;June;;;;;;
;;"Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks";;1-4;2020 IEEE International Conference for Innovation in Technology (INOCON);This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.;10.1109/INOCON50539.2020.9298378;;;;;2020;A Hybrid Approach to Data Pre-processing Methods;Desai, Vinod and H A, Dinesha;inproceedings;9298378;;Nov;;;;;;
;;"Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation";;1053-1057;2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI);With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.;10.1109/CISP-BMEI51763.2020.9263667;;;;;2020;Evaluation of Flight Test Data Quality Based on Rough Set Theory;Xiangwei, Kong;inproceedings;9263667;;Oct;;;;;;
;;"Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality";;1-6;2018 IEEE AUTOTESTCON;Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.;10.1109/AUTEST.2018.8532518;;;;;2018;Measuring Manufacturing Test Data Analysis Quality;Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy;inproceedings;8532518;;Sep.;;1558-4550;;;;
;;"Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing";;319-323;2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA);This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.;10.1109/ICBDA.2018.8367700;;;;;2018;Verification method of data quality in science and technology cloud in Shaanxi province;Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan;inproceedings;8367700;;March;;;;;;
;;"Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality";;1-5;2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD);The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.;10.1109/ICABCD.2018.8465132;;;;;2018;Improve Decision Making Towards Universities Performance Through Big Data Analytics;Segooa, Mmatshuene Anna and Kalema, Billy Mathias;inproceedings;8465132;;Aug;;;;;;
;;"Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality";;33-39;;Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.;10.23919/CJEE.2019.000025;;;;;2019;A missing power data filling method based on improved random forest algorithm;Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang;article;8950481;;Dec;Chinese Journal of Electrical Engineering;2096-1529;4;5;;
;;"Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning";;238-242;2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA);Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.;10.1109/CAIBDA53561.2021.00057;;;;;2021;Research on instance-level data cleaning technology;Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong;inproceedings;9545944;;May;;;;;;
;;"Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data";;74-77;2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT IoT (HONET-ICT);Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.;10.1109/HONET.2017.8102206;;;;;2017;Benefits of SDN for Big data applications;Alqarni, Mohammed A.;inproceedings;8102206;;Oct;;1949-4106;;;;
;;"Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality";;291-296;Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014);Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.;10.1109/IRI.2014.7051902;;;;;2014;Towards a technology roadmap for big data applications in the healthcare domain;Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina;inproceedings;7051902;;Aug;;;;;;
;;"Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations";;76-83;2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET);In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.;10.1109/MET.2019.00019;;;;;2019;Addressing Data Quality Problems with Metamorphic Data Relations;Auer, Florian and Felderer, Michael;inproceedings;8785536;;May;;;;;;
;;"Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems";;70-73;;"Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake ""work"".";10.1109/MIS.2014.82;;;;;2014;Embedding AI and Crowdsourcing in the Big Data Lake;O'Leary, Daniel E.;article;6949519;;Sep.;IEEE Intelligent Systems;1941-1294;5;29;;
;;"Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance";;2588-2594;2017 IEEE International Conference on Big Data (Big Data);Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.;10.1109/BigData.2017.8258218;;;;;2017;Identifying and mitigating risks to the quality of open data in the post-truth era;Colborne, Adrienne and Smit, Michael;inproceedings;8258218;;Dec;;;;;;
;;"Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees";;193-197;2019 IEEE 4th International Conference on Big Data Analytics (ICBDA);Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.;10.1109/ICBDA.2019.8713218;;;;;2019;An Automated Big Data Accuracy Assessment Tool;Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin;inproceedings;8713218;;March;;;;;;
;;"Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection";;1450-1452;2021 International Conference on Information and Communication Technology Convergence (ICTC);Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.;10.1109/ICTC52510.2021.9620761;;;;;2021;Mechanism of a big-data platform for residential heat energy consumption;Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon;inproceedings;9620761;;Oct;;2162-1233;;;;
;;"Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM";;181-186;2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII);The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.;10.1109/ICIICII.2016.0052;;;;;2016;Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry;Li, Tao and He, Yihai and Zhu, Chunling;inproceedings;7823519;;Dec;;;;;;
;;"Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process";;940-944;2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC);The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.;10.1109/ITOEC.2018.8740576;;;;;2018;Medical Data Quality Assessment Model Based on Credibility Analysis;Zan, Songting and Zhang, Xu;inproceedings;8740576;;Dec;;;;;;
;;"Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data";;248-255;2017 IEEE 13th International Conference on e-Science (e-Science);Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.;10.1109/eScience.2017.38;;;;;2017;ScienceDB: A Public Multidisciplinary Research Data Repository for eScience;Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang;inproceedings;8109143;;Oct;;;;;;
;;"Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop";;16-25;2014 IEEE/ACM International Symposium on Big Data Computing;In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.;10.1109/BDC.2014.10;;;;;2014;A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning;Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay;inproceedings;7321725;;Dec;;;;;;
;;"Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data";;811-815;2021 International Conference on Information Technology (ICIT);In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.;10.1109/ICIT52682.2021.9491629;;;;;2021;Recent Quality Models in BigData Applications;Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi;inproceedings;9491629;;July;;;;;;
;;"Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments";;91-91;2014 IEEE 18th International Enterprise Distributed Object Computing Conference;Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.;10.1109/EDOC.2014.21;;;;;2014;Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective;Ludwig, Heiko;inproceedings;6972054;;Sep.;;1541-7719;;;;
;;"Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization";;64-73;2014 IEEE/ACM International Symposium on Big Data Computing;We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.;10.1109/BDC.2014.11;;;;;2014;A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung;Ahnn, Jong Hoon;inproceedings;7321730;;Dec;;;;;;
;;"Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation";;140-147;2017 IEEE International Conference on Services Computing (SCC);Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.;10.1109/SCC.2017.25;;;;;2017;Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service;Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat;inproceedings;8034978;;June;;2474-2473;;;;
;;"Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform";;229-233;2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE);Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.;10.1109/ICECE51594.2020.9352886;;;;;2020;Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform;Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu;inproceedings;9352886;;Dec;;;;;;
;;"Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS";;149-156;2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2);Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.;10.1109/SC2.2017.30;;;;;2017;Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements;Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal;inproceedings;8315370;;Nov;;;;;;
;;"Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation";;43-46;2015 International Conference on Communication Networks (ICCN);The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.;10.1109/ICCN.2015.9;;;;;2015;Analysis and performance improvement of K-means clustering in big data environment;Rathore, Purva and Shukla, Deepak;inproceedings;7507493;;Nov;;;;;;
;;"Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning";;209-213;2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC);The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.;10.1109/ICSGRC49013.2020.9232648;;;;;2020;A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data;Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur;inproceedings;9232648;;Aug;;;;;;
;;"Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality";;1-6;2021 International Conference on Computer Information Sciences (ICCOINS);A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.;10.1109/ICCOINS49721.2021.9497187;;;;;2021;Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data;Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak;inproceedings;9497187;;July;;;;;;
;;"Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index";;2386-2390;2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC);The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.;10.1109/IAEAC47372.2019.8997699;;;;;2019;Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation;Pan, Lingling and Liu, Jun and Li, Feng;inproceedings;8997699;;Dec;;2381-0947;;1;;
;;Big data analytics, Resource-based view, Data quality management, IT capability, Data usage;;387-394;;Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.;https://doi.org/10.1016/j.ijinfomgt.2014.02.002;https://www.sciencedirect.com/science/article/pii/S0268401214000127;;;;2014;Data quality management, data usage experience and acquisition intention of big data analytics;Ohbyung Kwon and Namyeon Lee and Bongsik Shin;article;KWON2014387;;;International Journal of Information Management;0268-4012;3;34;;
;;Data quality, Big data, Context-awareness, Data profiling, DQ assessment;;548-562;;Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.;https://doi.org/10.1016/j.future.2018.07.014;https://www.sciencedirect.com/science/article/pii/S0167739X17329151;;;;2018;Context-aware data quality assessment for big data;Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali;article;ARDAGNA2018548;;;Future Generation Computer Systems;0167-739X;;89;;
;;Data Quality, Big Data, Measurement, Quality-in-Use, Model;;123-130;;Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.;https://doi.org/10.1016/j.future.2015.11.024;https://www.sciencedirect.com/science/article/pii/S0167739X15003817;;;;2016;A Data Quality in Use model for Big Data;Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini;article;MERINO2016123;;;Future Generation Computer Systems;0167-739X;;63;;Modeling and Management for Big Data Analytics and Visualization
;;Big data, Data quality and error, Data ethnics, Spatial information sciences;;134-142;;The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.;https://doi.org/10.1016/j.isprsjprs.2015.11.006;https://www.sciencedirect.com/science/article/pii/S0924271615002567;;;;2016;Rethinking big data: A review on the data quality and usage issues;Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu;article;LIU2016134;;;ISPRS Journal of Photogrammetry and Remote Sensing;0924-2716;;115;;Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'
;;Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining;;803-810;;Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.;https://doi.org/10.1016/j.procs.2019.11.007;https://www.sciencedirect.com/science/article/pii/S1877050919317077;;;;2019;The Impact of Big Data Quality on Sentiment Analysis Approaches;Imane El Alaoui and Youssef Gahi;article;ALAOUI2019803;;;Procedia Computer Science;1877-0509;;160;;The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops
;;data quality assessment, system identification, big data, Industry 4.0, soft sensors;;104-113;;As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.;https://doi.org/10.1016/j.ifacol.2020.12.103;https://www.sciencedirect.com/science/article/pii/S2405896320303591;;;;2020;Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0;Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov;article;SHARDT2020104;;;IFAC-PapersOnLine;2405-8963;2;53;;21st IFAC World Congress
;;Big data, data quality, outlier, Sustainable Development Goals;;685-692;;Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.;https://doi.org/10.1016/j.procs.2021.12.189;https://www.sciencedirect.com/science/article/pii/S1877050921024133;;;;2022;Diagnostic analysis for outlier detection in big data analytics;Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon};article;RIDZUAN2022685;;;Procedia Computer Science;1877-0509;;197;;Sixth Information Systems International Conference (ISICO 2021)
;;Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics;;1-13;;With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.;https://doi.org/10.1016/j.jprocont.2018.04.004;https://www.sciencedirect.com/science/article/pii/S0959152418300660;;;;2018;Big data quality prediction in the process industry: A distributed parallel modeling framework;Le Yao and Zhiqiang Ge;article;YAO20181;;;Journal of Process Control;0959-1524;;68;;
;;Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory;;103141;;Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.;https://doi.org/10.1016/j.im.2019.01.003;https://www.sciencedirect.com/science/article/pii/S0378720617308662;;;;2020;Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?;Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira;article;CORTEREAL2020103141;;;Information & Management;0378-7206;1;57;;Big data and business analytics: A research agenda for realizing business value
;;Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy;;22-38;;To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.;https://doi.org/10.1016/j.jhtm.2022.02.026;https://www.sciencedirect.com/science/article/pii/S1447677022000389;;;;2022;Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view;Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu;article;HORNG202222;;;Journal of Hospitality and Tourism Management;1447-6770;;51;;
;;Big data, Knowledge teaching evaluation, Performance management;;100197;;With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.;https://doi.org/10.1016/j.jik.2022.100197;https://www.sciencedirect.com/science/article/pii/S2444569X22000373;;;;2022;Review on A big data-based innovative knowledge teaching evaluation system in universities;Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan;article;XIN2022100197;;;Journal of Innovation & Knowledge;2444-569X;3;7;;
;;;;1947-1953;;"ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.";https://doi.org/10.1039/d1np00061f;https://www.sciencedirect.com/science/article/pii/S0265056822008856;;;;2021;Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality;Nadja B. Cech and Marnix H. Medema and Jon Clardy;article;CECH20211947;;;Natural Product Reports;0265-0568;11;38;;
;;Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory;;72-80;;Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.;https://doi.org/10.1016/j.ijpe.2014.04.018;https://www.sciencedirect.com/science/article/pii/S0925527314001339;;;;2014;Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications;Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer;article;HAZEN201472;;;International Journal of Production Economics;0925-5273;;154;;
;;Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability;;102093;;The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.;https://doi.org/10.1016/j.seta.2022.102093;https://www.sciencedirect.com/science/article/pii/S221313882200145X;;;;2022;AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities;El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon;article;OUAFIQ2022102093;;;Sustainable Energy Technologies and Assessments;2213-1388;;52;;
;;Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward;;107994;;"Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.";https://doi.org/10.1016/j.comnet.2021.107994;https://www.sciencedirect.com/science/article/pii/S138912862100116X;;;;2021;A blockchain-based trading system for big data;Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng;article;HU2021107994;;;Computer Networks;1389-1286;;191;;
;;Big data utilization, Data quality, Decision quality, Data diagnosticity;;38-49;;"Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.";https://doi.org/10.1016/j.dss.2019.03.008;https://www.sciencedirect.com/science/article/pii/S0167923619300466;;;;2019;Can big data improve firm decision quality? The role of data quality and data diagnosticity;Maryam Ghasemaghaei and Goran Calic;article;GHASEMAGHAEI201938;;;Decision Support Systems;0167-9236;;120;;
;;Credit card, Validation, LUHN, Big data, Bank;;532-537;;The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.;https://doi.org/10.1016/j.matpr.2021.05.597;https://www.sciencedirect.com/science/article/pii/S2214785321042243;;;;2022;Perspective of anomaly detection in big data for data quality improvement;Vinaya Keskar and Jyoti Yadav and Ajay Kumar;article;KESKAR2022532;;;Materials Today: Proceedings;2214-7853;;51;;CMAE'21
;;Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data;;132-147;;In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.;https://doi.org/10.1016/j.jpdc.2021.05.012;https://www.sciencedirect.com/science/article/pii/S0743731521001246;;;;2021;SparkDQ: Efficient generic big data quality management on distributed data-parallel computation;Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang;article;GU2021132;;;Journal of Parallel and Distributed Computing;0743-7315;;156;;
;;Big Data, Data Quality, Data Security, Trade-off between Quality, Security;;916-922;;"The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.";https://doi.org/10.1016/j.procs.2019.04.127;https://www.sciencedirect.com/science/article/pii/S1877050919305915;;;;2019;Big Data: Trade-off between Data Quality and Data Security;M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI;article;TALHA2019916;;;Procedia Computer Science;1877-0509;;151;;The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops
;;Data quality-aware queries, Big data computing, Empirical evaluation;;114858;;Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.;https://doi.org/10.1016/j.eswa.2021.114858;https://www.sciencedirect.com/science/article/pii/S0957417421002992;;;;2021;Experimenting with big data computing for scaling data quality-aware query processing;Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris;article;CISNEROSCABRERA2021114858;;;Expert Systems with Applications;0957-4174;;178;;
;;Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA;;374-387;;"Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).";https://doi.org/10.1016/j.future.2022.04.020;https://www.sciencedirect.com/science/article/pii/S0167739X22001492;;;;2022;Big data architecture for connected vehicles: Feedback and application examples from an automotive group;Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables;article;MOSTEFAOUI2022374;;;Future Generation Computer Systems;0167-739X;;134;;
;;Semantic-aware, Quality assessment, Image big data, IDSTH, SHR;;53-65;;"Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.";https://doi.org/10.1016/j.future.2019.07.063;https://www.sciencedirect.com/science/article/pii/S0167739X19302304;;;;2020;Semantic-aware data quality assessment for image big data;Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu;article;LIU202053;;;Future Generation Computer Systems;0167-739X;;102;;
;;Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory;;121222;;The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.;https://doi.org/10.1016/j.techfore.2021.121222;https://www.sciencedirect.com/science/article/pii/S0040162521006557;;;;2022;Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice;Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin;article;WEERASINGHE2022121222;;;Technological Forecasting and Social Change;0040-1625;;174;;
;;Data quality, Big data, Secondary data, Numerical data, Quality threshold;;113135;;An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.;https://doi.org/10.1016/j.dss.2019.113135;https://www.sciencedirect.com/science/article/pii/S0167923619301642;;;;2019;Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development;Anita Lee-Post and Ram Pakath;article;LEEPOST2019113135;;;Decision Support Systems;0167-9236;;126;;Perspectives on Numerical Data Quality in IS Research
;;Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security;;209-226;;Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.;https://doi.org/10.1016/j.future.2022.01.017;https://www.sciencedirect.com/science/article/pii/S0167739X22000243;;;;2022;A survey on blockchain for big data: Approaches, opportunities, and future directions;N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana;article;DEEPA2022209;;;Future Generation Computer Systems;0167-739X;;131;;
