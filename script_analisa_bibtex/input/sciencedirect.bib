@article{GU2021132,
title = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {132-147},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001246},
author = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
keywords = {Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data},
abstract = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.}
}
@article{CISNEROSCABRERA2021114858,
title = {Experimenting with big data computing for scaling data quality-aware query processing},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114858},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002992},
author = {Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris},
keywords = {Data quality-aware queries, Big data computing, Empirical evaluation},
abstract = {Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}